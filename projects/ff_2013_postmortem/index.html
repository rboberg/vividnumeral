<!DOCTYPE html>
<HTML>
<HEAD>
	<META content="IE=10.000" http-equiv="X-UA-Compatible">
	<META charset="utf-8">  <!--Always force latest IE rendering engine (even in intranet) & Chrome Frame Remove this if you use the .htaccess --> 
	<META http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<!--  Mobile viewport optimized: j.mp/bplateviewport --> 
	<META name="viewport" content="width=device-width, initial-scale=1.0">


	<TITLE>2013 Fantasy Post-Mortem</TITLE>   
	<META name="description" content="A look back at whether my systematic, quantitative approach to the 2013 Fantasy Football draft was actually any good.">   	
	<META name="date_posted" content="1986/12/05">
	<META name="author" content="Ross Boberg">
	<LINK href="../../style/article.css" rel="stylesheet" type="text/css">
	<LINK href="webfiles/style.css" rel="stylesheet" type="text/css">
	
	
</HEAD>

<BODY>
	<DIV id="content">

		<DIV id="page_header">
			<H1>2013 Fantasy Football Post-Mortem</H1>
			<p><a href="../../">Home</a></p>
		</DIV>

		<DIV id="main_text">

			<h2>Nerd Level : 1</h2>

			<p>
			Before the 2013 Football season I wrote a couple pieces detailing a quantitative fantasy draft methodology. You can find the details <a href='../ff_rank'>here</a> , but, in short, I used player stat projections aggregated by <a href='http://www.fantasypros.com'>fantasypros.com</a> and some adjustments to compare players across positions to quantify each player's value. With the season in the rear-view mirror, I want to know whether it was worth the trouble.	
			</p>
			
			<p>
			I'm feeling pretty good about the model since all three of my fantasy teams were first or second this year. But that's a very small sample size and a ton of other factors that could effect that outcome (like luck). This article takes a more robust look at whether it was a waste of time.
			</p>
			
			<p>
			The punch line is that the expert estimate based draft ranks added between 4 and 7 points per game over just using ESPN's Average Draft Position to determine picks. That is statistically significant. I broke this article in to Nerd Levels: increasing degrees of method detail and quantitative complexity. So you can take my word that this method adds value, stop reading, and send me a congratulatory email or move on to Level 2.
			</p>

			<h2>Nerd Level : 2</h2>

			<p>
				To quantify the points added by my method, I ran a mock draft of a 12 team leage where 6 teams pick according to ESPN's pre-season average draft position (ADP) and 6 teams pick the the player with the best expert consensus value. I'll refer to the two types as ADP and Value Based going forward. Once the rosters were set, I calculated how many points each team would have scored over the sytem
			</p>

			<p>
				If you're unusually interested, intrepid, or bored you can find more detail on the draft methodology here **INSERT LINK TO DRAFT METHODOLOGY HERE**. You can also find the R code used on GitHub **INSERT rross GITHUB LINK**.
			</p>

			<h3>Broad Results</h3>

			<p>
				Once I have the points scored by each team there are a couple ways to translate that in to a weekly point difference between the two mehtods.
			</p>

			<ol>
				<li>
				Average the weekkly points of the players at each position for each strategy. Then multiply by the number of starters at each position. This way, the Value Based teams averaged 4.4 points per week more than the ADP teams.
				</li>
				<li>
				Same as method one, but instead of averaging the points of all players, only average the points of players that would have started. This way, the Value Based teams averaged 6.2 points per week more than the ADP teams.
				</li>
			</ol>
			<p>
				The second method throws out the worst outliers from each strategy's draft, so flukes like a player getting injured for the entire season in week 1 will not overly effect the results. This method is also more foregiving of high risk / high reward picks that don't pan out.
			</p>
			<p>
				With 6.2 points per week instead of 4.4, method two is more flattering of the Value Based method. The main reason for the difference is Josh Freeman. The the Value Based system drafted him, and his almost non-existant season gets included in method one but thrown out in method two, because there's no way he would have started. More on that later.
			</p>

			<p>
				It's possible I was just lucky that the Value Based approach added around 5 points per week. But it seems unlikely. This table summarizes the stastical significance of the results for each calculation method.
			</p>



			<div id='broad_table'>
				<table>
					<tr>
						<th colspan='100'>Weekly Points of Value Based over ADP</th>
					</tr>
					<tr>
						<th rowspan='2' class='tdtext'>Method</th>
						<th colspan='3' style='border:none'>95% Confidence Interval</th>
						<th rowspan='2' class='tdvalue'>Probability<br>Points > 0</th>
					</tr>
					<tr>
						<th class='tdvalue'>Low</th>
						<th class='tdvalue'>Mean</th>
						<th class='tdvalue'>High</th>
					</tr>
					<tr>
						<td class='tdtext'>(1) All Players</td>
						<td class='tdvalue'>-0.8</td>
						<td class='tdvalue'>4.4</td>
						<td class='tdvalue'>9.6</td>
						<td class='tdvalue'>95.6%</td>
					</tr>
					<tr>
						<td class='tdtext'>(2) Starters Only</td>
						<td class='tdvalue'>1.6</td>
						<td class='tdvalue'>6.2</td>
						<td class='tdvalue'>10.8</td>
						<td class='tdvalue'>99.7%</td>
					</tr>
				</table>
			</div>

			<p>
				The probability that the Value Based approach added positive points over ADP is greater than 95% for both calculation methods. 95% is usually good enough for scholarly articles, so I'll take it.
			</p>

			<h2> Nerd Level : 3</h2>

			<p>
				There's lots of questions to ask about how these extra points are being generated. How are the extra points being generated? What did the method do right? Where did it screw up?  Breaking down the points added by position is a way to start answering these questions.
			</p>

			<div id='postable_div'>
			<table id='postable'></table>
			</div>

			<p>
				The weak spot in the Value Based drafting is highlighted by the red in the table above.
				With only starters included, I added positive value at QB, but it's still the smallest of any position.
				What happened?
			</p>

			<p>
				This visualization helps answer that question by providing more granularity. Each bar is a draft pick, arranged by draft order from left to right. The size of each bar indicates how well each pick turned out, measured by how many more points that player scored than the worst player that would have started at that position in a 12 team league. The color indicates whether they were drafted by a Value Based team (my system) or an ADP based team.
			</p>

			<p>
				The chart is interactive. Hover over any bar and to see more information about player it represents. Use the buttons at the top to highlight only certain positions. Zoom in and out by clicking and dragging on the small chart at the bottom. 
			</p>

			<div id='draft_viz'>
				
			</div>


			<p>
				If you filter on QB's you can see that the two biggest bars are black, indicating that my Value Based system missed those players. Hover over and to see that they are Peyton Manning and Drew Brees. My method clearly did not anticipate the historic years those players would have.
			</p>
			<p>
				ADP correctly drafted the MVFP (most valuable fantasy player) and MVRP (most valuable reality player) Peyton Manning. I also missed 3MVFP (3rd most valuable fantasy player) Drew Brees (my finance background makes awkward acronyms second nature). Those misses hurt my QB cred.
			</p>

			<p>
				However the next 10 or so QBs drafted all produced about the same value. I waited until late to draft those QBs while ADP took them early. That earned back a bit of my QB cred.
			</p>

			<p>
				The most negative QB bar, Josh Freeman, was drafted by my Value Based method.
				That pick accounts for the QB improvement in "Starters Only" method. Providing almost no points and the high point scoring positiong, he was by far the worst pick of the draft. Still given he was a late round pick drafted as a back up QB, I think excluding him gives a better assesment of the system's overall value.
			</p>

			<p>
				The Value Based picking made up for the QB deficiencies at the otehr positions. It picked the most top 3 RB's, Jamal Charles, LeSean McCoy, and Matt Forte. It picked six of the top eight WR's, in particular picking the most valueable WR, Josh Gordon, in the 9th round.
			</p>

			<h2> Nerd Level : 4</h2>

			<p>
				The number of points garnered at each position can be broken down in to two sources: Position Preference and Player Choice.
			</p>

			<h3>(1) Position Preference</h3>

			<p>
				Position Preference is the value derived from drafting a position early or late.
				For example, if I draft running backs early I should get more points from running backs than my opposition, regardless of how good I am at choosing between running backs.
				But, I give up points at other positions by going after RBs aggressively.
			</p>

			<p>
				However, I can net add value throughout my roster by drafting a position early if the skill generally drops off fastest at that position. In other words, if the marginal value of drafting early at that position is greater than the other positions. 
			</p>

			<p>
				To calcualte the difference in points between the two draft strategies attributable to position preference, you need to know (A) where each strategy drafted a particular position on average (Mean Draft Position) and (B) how many points it is worth to draft a player one pick earlier (Marginal Points).
			</p>
			<p>
				Calculating the Mean Draft Position for each strategy and position is straightforward (and self defining), just take the mean of all the draft picks. It tells you how early a strategy type picked that position on average.
			</p>
			<p>
				I used a linear regression of the Points Scored on the Draft Position to estimate the Marginal Points for each position. That value tells you how important it was to draft a player early. A higher number indicates it was more important to draft that position early. 
			</p>
			<p>
				This table summarizes the results. It was most important to pick QB's early (they had the highest Marginal Points). Unfortunately, the Value Based system did not do that. That results in the big red bar you see in the table indicating that I gave up a lot of value by not drafting QB's early enough.
			</p>
			<p>
				Generally, this big negative should be offset by a positive somewhere. If I wasn't picking QB's I was picking something else. 
				But because RBs, which had less marginal value than QB's, the Value Based strategy gave up some points on average by not picking QBs as early as the ADP strategy. 
			</p>
			<p>
				In total the Value Based system lost 0.58 points by not going after QB's aggressively enough early.
			</p>


			<div id='preftable_div'>
			<table id='preftable'></table>
			</div>

			<h3>(2) Player Choice</h3>

			<p>
				Player Choice is the value derived from drafting better players with the same pick as your opponents.
				For example, if I consistently pick better running backs one pick after other people in the league, then I am adding value by Player Choice. This is the number of points that cannot be explained by Position Preference.
			</p>

			<div id='choicetable_div'>
			<table id='choicetable'></table>
			</div>

			<h2>Lessons</h2>
				<p>
					This value based method seems to consistently beat just drafting according to the average draft position. It doesn't always smake the best choice, as can be seeing by missing out on gigantic years by Peyton Manning and Drew Brees, but it consistently adds value across positions and throughout the draft.
				</p>
				<p>
					I'll be using this methodology next year, maybe with some added wrinkles. This year I only used the average projection in ranking players. Information about the uncertainty of projections might add value. For example, if the distribution of expert projections are all over the place, that might suggest the player is a risky pick and I can downgrade the rank accordingly.
				</p>
				<p>
					Another area where I could improve would be more dynamic benchmarking. Comparing players within positions is pretty easy. If Adrian Peterson is projected to have more points than Arian Foster, take Adrian Peterson. But choosing BETWEEN positions is difficult.
				</p>
				<p>
					To choose whether a player at one position is better than another, you have to adjust their projected points scored to try to make their values comparable. This year I used the worst player I might end up with at a position as the benchmark, but that can be improved.
				</p>
				<p>
					A better benchmark might be the player I can get next round if I refrain from drafting this player this round. The intuition is, if the first 10 wide receivers are all projected about the same, it makes sense to take another position and wait until next round to take a wide receiver.
				</p>
		</DIV>
	</DIV>


<!-- SCRIPT GOES HERE-->
	<script src="webfiles/d3.js"></script>
	<script src="webfiles/underscore-min.js"></script>
	<script src="webfiles/jquery-min.js"></script>
	<script src="webfiles/main.js"></script>

</BODY>
</HTML>