<!DOCTYPE html PUBLIC "" "">
<HTML>
<HEAD>
	<META content="IE=10.000" http-equiv="X-UA-Compatible">
	<META charset="utf-8">  <!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame Remove this if you use the .htaccess --> 
	<META http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<!--  Mobile viewport optimized: j.mp/bplateviewport --> 
	<META name="viewport" content="width=device-width, initial-scale=1.0">
	<META name="GENERATOR" content="MSHTML 10.00.9200.16635">


	<TITLE>2013 Fantasy Post-Mortem</TITLE>   
	<META name="description" content="A look back at whether my systematic, quantitative approach to the 2013 Fantasy Football draft was actually any good.">   	
	<META name="date_posted" content="1986/12/05">
	<META name="author" content="Ross Boberg">
	<LINK href="../../style/article.css" rel="stylesheet" type="text/css"></LINK>
	<LINK href="webfiles/style.css" rel="stylesheet" type="text/css"></LINK>
	
	
</HEAD>

<BODY>
	<DIV id="content">

		<DIV id="page_header">
			<H1>2013 Fantasy Football Post-Mortem</H1>
			<p><a href="../../">Home</a></p>
		</DIV>

		<DIV id="main_text">

			<h2>Nerd Level : 1</h2>

			<p>
			Before the 2013 Football season I wrote a couple pieces detailing a quantitative fantasy draft methodology. You can find the details <a href='../ff_rank'>here</a> , but, in short, I used player stat projections aggregated by <a href='http://www.fantasypros.com'>fantasypros.com</a> and some adjustments to compare players across positions to quantify each player's value. With the season in the rear-view mirror, I want to know whether it was worth the trouble.	
			</p>
			
			<p>
			I'm feeling pretty good about the model since all three of my fantasy teams were first or second this year. But that's a very small sample size and a ton of other factors that could effect that outcome (like luck). This article takes a more robust look at whether it was a waste of time.
			</p>
			
			<p>
			The punch line is that the expert estimate based draft ranks added between 4 and 7 points per game over just using ESPN's Average Draft Position to determine picks. That is statistically significant. I broke this article in to Nerd Levels: increasing degrees of method detail and quantitative complexity. So you can take my word that this method adds value, stop reading, and send me a congratulatory email or move on to Level 2.
			</p>

			<h2>Nerd Level : 2</h2>

			<p>
				To quantify the points added by my method, I ran a mock draft of a 12 team leage where 6 teams pick according to ESPN's pre-season average draft position (ADP) and 6 teams pick the the player with the best expert consensus value. I'll refer to the two types as ADP and Value Based going forward. Once the rosters were set, I calculated how many points each team would have scored over the sytem
			</p>

			<p>
				If you're unusually interested, intrepid, or bored you can find more detail on the draft methodology here **INSERT LINK TO DRAFT METHODOLOGY HERE**. You can also find the R code used on GitHub **INSERT rross GITHUB LINK**.
			</p>

			<h3>Broad Results</h3>

			<p>
				Once I have the points scored by each team there are a couple ways to translate that in to a weekly point difference between the two mehtods.
			</p>

			<ol>
				<li>
				Average the weekkly points of the players at each position for each strategy. Then multiply by the number of starters at each position. This way, the Value Based teams averaged 4.4 points per week more than the ADP teams.
				</li>
				<li>
				Same as method one, but instead of averaging the points of all players, only average the points of players that would have started. This way, the Value Based teams averaged 6.2 points per week more than the ADP teams.
				</li>
			</ol>
			<p>
				The second method throws out the worst outliers from each strategy's draft, so flukes like a player getting injured for the entire season in week 1 will not overly effect the results. This method is also more foregiving of high risk / high reward picks that don't pan out.
			</p>
			<p>
				With 6.2 points per week instead of 4.4, method two is more flattering of the Value Based method. The main reason for the difference is Josh Freeman. The the Value Based system drafted him, and his almost non-existant season gets included in method one but thrown out in method two, because there's no way he would have started. More on that later.
			</p>

			<p>
				It's possible I was just lucky that the Value Based approach added around 5 points per week. But it seems unlikely. This table summarizes the stastical significance of the results for each calculation method.
			</p>



			<div id='broad_table'>
				<table>
					<tr>
						<th colspan='100'>Weekly Points of Value Based over ADP</th>
					</tr>
					<tr>
						<th rowspan='2' class='tdtext'>Method</th>
						<th colspan='3' style='border:none'>95% Confidence Interval</th>
						<th rowspan='2' class='tdvalue'>Probability<br>Points > 0</th>
					</tr>
					<tr>
						<th class='tdvalue'>Low</th>
						<th class='tdvalue'>Mean</th>
						<th class='tdvalue'>High</th>
					</tr>
					<tr>
						<td class='tdtext'>(1) All Players</td>
						<td class='tdvalue'>-0.8</td>
						<td class='tdvalue'>4.4</td>
						<td class='tdvalue'>9.6</td>
						<td class='tdvalue'>95.6%</td>
					</tr>
					<tr>
						<td class='tdtext'>(2) Starters Only</td>
						<td class='tdvalue'>1.6</td>
						<td class='tdvalue'>6.2</td>
						<td class='tdvalue'>10.8</td>
						<td class='tdvalue'>99.7%</td>
					</tr>
				</table>
			</div>

			<p>
				The probability that the Value Based approach added positive points over ADP is greater than 95% for both calculation methods. 95% is usually good enough for scholarly articles, so I'll take it.
			</p>

			<h2> Nerd Level : 3</h3>

		<DIV>
	</DIV>
</BODY>
</HTML>