<!DOCTYPE html PUBLIC "" "">
<HTML>
<HEAD>
	<META content="IE=10.000" http-equiv="X-UA-Compatible">
	<META charset="utf-8">  <!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame Remove this if you use the .htaccess --> 
	<META http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	<!--  Mobile viewport optimized: j.mp/bplateviewport --> 
	<META name="viewport" content="width=device-width, initial-scale=1.0">
	<META name="GENERATOR" content="MSHTML 10.00.9200.16635">


	<TITLE>Postmortem of a World Cup Postmortem</TITLE>   
	<META name="description" content="I was kindly advised of an flaw in my original World Cup postmortem. So it's time to put the rubber gloves back on and disect the postmortem itself.">   	
	<META name="date_posted" content="2014/09/10">
	<META name="author" content="Ross Boberg">
	<LINK href="../../style/article.css" rel="stylesheet" type="text/css"/>
	<LINK href="webfiles/style.css" rel="stylesheet" type="text/css"></LINK>
	<LINK rel="shortcut icon" href="../../favicon.ico" />
	
</HEAD>

<BODY>
	<DIV id="content">

		<DIV id="page_header">
			<H1>Postmortem of a World Cup Postmortem</H1>
			<p><a class='author'>by Ross Boberg</a> / <a href="../../">Home</a></p>
		</DIV>

		<DIV id="main_text">
			<h2>Cut by Occam's Razor</h2

			<p>
				Occam's razor is a popular scientific rule of thumb. If you are faced with several theories and can't choose between them, use the simplest one. It's a fancy version of <a href='http://www.tvfanatic.com/quotes/michael-always-says-k-i-s-s-keep-it-simple-stupid-great-advic/'>K.I.S.S.</a>.
			</p>
			<p>
				I love that advice and always try to err on the side of parsimony. But in my <a href='http://www.vividnumeral.com/content/projects/2014_wc_group_result/'>World Cup postmortem</a>, I crossed the line from simple to stupid. The scoring I used to evaluate the forecasts of World Cup outcomes was flawed.
			</p>
			<p>
				I measured the error of a forecast as the absolute value of the outcome (100% if the team advanced, 0% if they didn't) minus the forecasted probability of advancing. If I said a team had a 75% chance of advancing and they did advance, the error was 25%. If they did not advance the error was 75%.
			</p>
			

			<table id='example_table'>
				<tr>
					<th class='tdtext'>Example</th>
					<th class='tdvalue'>Case1</th>
					<th class='tdvalue'>Case2</th>
				</tr>
				<tr>
					<td class='tdtext'>Forecast Advance Probability (A)</td>
					<td class='tdvalue'>90%</td>
					<td class='tdvalue'>100%</td>
				</tr>
				<tr>
					<td class='tdtext'>True Advance Probability (B)</td>
					<td class='tdvalue'>90%</td>
					<td class='tdvalue'>90%</td>
				</tr>
				<tr>
					<td class='tdtext'>Error if Advance (C = 1-A)</td>
					<td class='tdvalue'>10%</td>
					<td class='tdvalue'>0%</td>
				</tr>
				<tr>
					<td class='tdtext'>True Fail Probability (D = 1-B)</td>
					<td class='tdvalue'>10%</td>
					<td class='tdvalue'>10%</td>
				</tr>
				<tr>
					<td class='tdtext'>Error if Fail (E = A)</td>
					<td class='tdvalue'>90%</td>
					<td class='tdvalue'>100%</td>
				</tr>
				<tr class='trtotal'>
					<td class='tdtext'>Expected Error (F = B&#215;C + D&#215;E)</td>
					<td class='tdvalue'>18%</td>
					<td class='tdvalue'>10%</td>
				</tr>
			</table>
			<p>
				An example helps illustrate why that is a bad measure. Say that the truth is that a particular team would advance 90% of the time. Then 90% is the best possible guess and should have the lowest expected error.
			</p>

			<p>
				If I guess 90% then my absolute error will be 10% 90% of the time (when the team does win) and 90% 10% of the time (when the team doesn't). The average expected error will be 18%.
			</p>
			<p>
				A guess of 100% is wrong, so the expected error shoud be bigger. If I guess 100%, then my error would be 0% 90% of the time (when the team won) and 100% 10% of the time (when the team lost). The average expected error will be 10%.
			</p>

			<p>
				So I got a better score (lower error) for a worse guess?! That should not happen.
			</p>

			<h2>Improper Behavior</h2>
			<p>
				The mean absolute error did not yield the best score when the probability was guessed correctly.
			</p>
			<p>
				That's a big problem. In terms of decision theory, my scoring rule was not <a href='http://en.wikipedia.org/wiki/Scoring_rule'>proper</a>. Trying to do something more intuitive and easier to interpret, I mistakenly sacrificed accuracy.
			</p>
			<p>
				Graphically you can see the problem below. Each chart represents a different version of reality, where a team had a 20%, 50%, and 80% chance of advancing respectively. The blue line shows the expected score for every possible probability forecast according to the difference rule I used in the original article (absolute value of outcome minus forecast).
			</p>
			<p>
				You can see that when 20% is the real probability, the forecast with the best (highest) expected score was 0%. For the rule to be proper, the expected score should peak at 20%. In the 50% case, all forecasts had the same expected score when it should uniquely peak at 50% to be a "proper" scoring rule.
			</p>
			<div id="score_img">
				<img border="0" src="webfiles/score_algo_comp.png" alt="All Positions" width="600" height="300">
			</div>
			<br>
			<p>
				On the same charts I drew a line for the <a href='http://en.wikipedia.org/wiki/Scoring_rule#Logarithmic_scoring_rule'>log score</a> in red, a strictly proper scoring rule defined: Actual&#215;ln(Forecast) + (1-Actual)&#215;ln(1-Forecast). (Nerd note: I exponentiated the log score in the chart so that it takes a value between 0 and 1).
			</p>
			<p>
				Notice that the log score peaks in all the right places. A 20% forecast gives the highest expected score if reality is a 20% probability. A 50% forecast gives the highest expected score if reality is a 50% probability. An 80% forecast gives the highest expected score if reality is a 80% probability. That means the rule is proper! Hooray.
			</p>

			<h2>Results using the Log Score</h2>
			<p>
				If I calculate the log score for each forecast and average for each forecaster, VividNumeral still comes out ahead. The mean log score is -0.59 for VividNumeral, higher than -0.62 for FiveThirtyEight. Those numbers are really hard to interpret, but if I use e<sup>log score</sup> they are 55.2% and 53.7% respectively where a na√Øve 50% forecast for each would have a score of 50% to provide some context.
			</p>

			<p>
				My predictions were better, but it's still hard to gauge how much better and the degree to which that would be consistent over time. I hope this post proves I'm open to suggestions of better ways of doing things. If anyone has a better way to compare these forecasts, feel free to email me ideas: <a href='mailto:ross@vividnumeral.com'>ross@vividnumeral.com</a>.
			</p>

			<p>
				Finally, a big thank you to Ben for raising questions about the original postmortem.
			</p>


		<DIV>
	</DIV>
</BODY>
</HTML>